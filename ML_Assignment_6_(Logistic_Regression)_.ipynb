{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Assignment - 6 (Logistic Regression) .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/95871abhinandankumar/Machine-learning/blob/main/ML_Assignment_6_(Logistic_Regression)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUXfhtsoi9kO"
      },
      "source": [
        "https://github.com/codePerfectPlus/Logistic_Regression_From_Scratch/blob/master/IRIS%20Logistic%20Regression%20From%20Scratch.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ySD4bt6B16"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGCMKX4VSQMa"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zID8FIjj6xfv",
        "outputId": "1b9b2cd3-e823-40ee-c871-fc6732dec464"
      },
      "source": [
        "print(\"The dictonary keys are associated with data:\\n\", iris.keys())\n",
        "print(\"-\"*100)\n",
        "print(\"Data:\\n\", iris['data'])\n",
        "print(\"-\"*100)\n",
        "print(\"Name of the features (Independent features): \\n\", iris['feature_names'])\n",
        "print(\"-\"*100)\n",
        "print(\"Name of the dependent features: \", iris['target_names'])\n",
        "print(\"-\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dictonary keys are associated with data:\n",
            " dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Data:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Name of the features (Independent features): \n",
            " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Name of the dependent features:  ['setosa' 'versicolor' 'virginica']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlbsTGR3YgoX"
      },
      "source": [
        "# 1. Pre-process the data to use only 2 classes (Iris-Setosa and Iris-Versicolour)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWGXnGKm6lUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e97413f-3ea5-4662-e387-5b3cafb68355"
      },
      "source": [
        "y_actual = iris['target']\n",
        "y_actual = list(y_actual)\n",
        "counter_setosa = 0\n",
        "counter_versicolor = 0\n",
        "counter_virginica = 0\n",
        "for i in range(0,len(y_actual)):\n",
        "  if(y_actual[i]==0):\n",
        "    counter_setosa = counter_setosa  + 1 \n",
        "  if(y_actual[i]==1):\n",
        "    counter_versicolor = counter_versicolor + 1\n",
        "  if(y_actual[i]==2):\n",
        "    counter_virginica = counter_virginica + 1\n",
        "print(\"Total no of patterns in setosa class\",counter_setosa)\n",
        "print(\"Total no of patterns in setosa class\",counter_versicolor)\n",
        "print(\"Total no of patterns in setosa class\",counter_virginica)\n",
        "# take only first 100 patterns\n",
        "for i in range(0,50):\n",
        "  y_actual.remove(2)\n",
        "# covert it to an numpy array \n",
        "y_actual = np.array(y_actual)\n",
        "print(y_actual.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no of patterns in setosa class 50\n",
            "Total no of patterns in setosa class 50\n",
            "Total no of patterns in setosa class 50\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPbL34Wk7l9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff9c933-8e7e-4168-c481-227b43109ac5"
      },
      "source": [
        "# take only first 100 patterns for the binary classification \n",
        "features = list(iris['data'])\n",
        "print(len(features))\n",
        "del features[100:]\n",
        "print(len(features))\n",
        "features = np.array(features)\n",
        "print(\"No of the feature vectors: %d\"%features.shape[1])\n",
        "print(\"No of the Patterns: %d\"%features.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n",
            "100\n",
            "No of the feature vectors: 4\n",
            "No of the Patterns: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDhY9YtGYmdw"
      },
      "source": [
        "# 2. Divide the dataset into train, validation and test with the dataset division ratio of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6KMBvswoh5h"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(features)\n",
        "features = scaler.transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQeOnF8A7s9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c0dade-af79-412f-f4e9-18d76cbb1ec2"
      },
      "source": [
        "# Train test validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "# train(70%), validation (10%), and test(20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features,y_actual , test_size=0.3, random_state=random.randint(30,100))\n",
        "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.14, random_state=random.randint(30,100))\n",
        "print(\"Size of the train dataset:\",X_train.shape)\n",
        "print(\"Size of the validation dataset:\",X_validation.shape)\n",
        "print(\"Size of the test dataset:\",X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the train dataset: (70, 4)\n",
            "Size of the validation dataset: (5, 4)\n",
            "Size of the test dataset: (25, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKYWsdcCZ4Vu"
      },
      "source": [
        "# 3. Hyperparameter tuning on the validation set is required to be done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeRqs2xUa3OL"
      },
      "source": [
        "3.1. Sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Y-xrJbaz-O"
      },
      "source": [
        "def sigmoid(x):\n",
        "  z = 1/(1 + np.exp(-x))\n",
        "  return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIIX9glWaWjL"
      },
      "source": [
        "3.2. Gradient Decent Algorithem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA1o66YO-sf9"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "def gradient_descent(alpha, x, y, roh = 0.0001, max_iter=10):\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    m = x.shape[0]\n",
        "    n = x.shape[1]\n",
        "    # Initial Weights\n",
        "    weight = np.ones(n+1)\n",
        "    hypo = np.ones(m)\n",
        "    random.seed(10) # for the same weight intialization\n",
        "    for i in range(0, n+1):\n",
        "      weight[i] = random.uniform(-0.3, 0.3)\n",
        "    # calculate the hypothesis value with the intial weights\n",
        "    k = 0\n",
        "    for i in range(0, m):\n",
        "      rest = 0 \n",
        "      for j in range(1, n+1):\n",
        "        rest = rest + weight[j]*x[i][j-1]\n",
        "      hypo[i] = 1*weight[0] + rest\n",
        "      hypo[i] = sigmoid(hypo[i]) # call sigmoid function for the hypothesis\n",
        "    # implement log loss function of the error\n",
        "    temp_sum = 0\n",
        "    for i in range(0, x.shape[0]):\n",
        "      temp_sum = temp_sum + ((y[i]*math.log(hypo[i])) + (1-y[i])*math.log(hypo[i]))\n",
        "    log_loss = -(1/x.shape[0])*temp_sum\n",
        "    while not converged:\n",
        "        error = 0\n",
        "        # Update the weight\n",
        "        hypothesis = 0\n",
        "        temp_out = 0\n",
        "        for i in range(0, m):\n",
        "          rest = 0 \n",
        "          for j in range(1, n+1):\n",
        "            rest = rest + weight[j]*x[i][j-1]\n",
        "          hypo[i] = 1*weight[0] + rest\n",
        "          hypo[i] = sigmoid(hypo[i])\n",
        "        for i in range(0, m):\n",
        "          weight[0] = (hypo[i] - y[i])*1\n",
        "        for i in range(0, m): # fix the row\n",
        "          hypothesis = 0\n",
        "          for j in range(0, n+1): # fix the column\n",
        "            hypothesis = hypothesis + weight[j+1]*x[i][j] # hypothesis calculation\n",
        "          if()\n",
        "          temp_out = temp_out + (weight[0] + hypothesis - y[j])*1 # as xi,0 = 1\n",
        "        my_sum[0] = temp_out # for the weight 0\n",
        "        # for rest of the weights\n",
        "        for k in range(1, n+1):\n",
        "          hypothesis = weight[0]\n",
        "          temp_out = 0\n",
        "          for i in range(0, m): # vary the row\n",
        "            for j in range(0, n):\n",
        "              hypothesis = hypothesis + weight[j+1]*x[i][j]\n",
        "            temp_out = temp_out + (weight[1] + temp_in - y[j])*x[i][k]\n",
        "          my_sum[k] = temp_out\n",
        "        # weight updatation\n",
        "        for j in range(0,n+1):\n",
        "          weight[j] = weight[j] - (alpha/m) * my_sum[j]\n",
        "        return weight\n",
        "        # Calculate the Mean Squared Error\n",
        "        for i in range(0,m): # vary the row\n",
        "          for j in range(0,x.shape[1]):\n",
        "            temp_in = temp_in + weight[j+1]*x[i][j]\n",
        "          error = error + (weight[0] + temp_in - y[i])**2 # total error\n",
        "        print(error)\n",
        "        if abs(total_error-error) <= ep:\n",
        "            print(total_error)\n",
        "            print(error)\n",
        "            print('Converged, iterations: ', iter, '!!!')\n",
        "            converged = True\n",
        "        total_error = error  # update error \n",
        "        iter += 1  # update iter\n",
        "        if iter == max_iter:\n",
        "            print('Max interactions exceeded!')\n",
        "            converged = True\n",
        "    return weight, error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "6GOJ3HG-c_CS",
        "outputId": "cc49c132-f70f-4220-aa90-6537f2f9ec00"
      },
      "source": [
        "shyam = gradient_descent(0.01, X_train, y_train, roh=0.00001, max_iter=10)\n",
        "print(shyam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-ab4953c9a2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshyam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshyam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-32677588d467>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(alpha, x, y, roh, max_iter)\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# fix the column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypothesis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# hypothesis calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m           \u001b[0mtemp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# as xi,0 = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmy_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_out\u001b[0m \u001b[0;31m# for the weight 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "mHbUQZmhmfTq",
        "outputId": "ae701455-3415-4b7f-b58c-b0a65943ff8f"
      },
      "source": [
        "y_test = [1, 2, 3]\n",
        "y = y_test[0] # y = 1\n",
        "print(y[0]) # this line will fail"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b308364dbc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# y = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this line will fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWiKmZuU-slp"
      },
      "source": [
        "alpha = [ 0.1, 0.001, 0.0001, 0.5, 1.0]\n",
        "mse = np.ones(5)\n",
        "weight = np.ones(14)\n",
        "for i in range(5):\n",
        "  weight,mse[i] = gradient_descent(alpha[i], X_train, y_train, ep=0, max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MhPJ-KB-soV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwqTcJ9l-sqy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bi_uIq_-stQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}